01. 컨텐츠기반 추천시스템이란?

컨텐츠가 비슷한 아이템을 추천하다. 예르 들어, 사용자가 과거 경험했던 아이템 주 비슷한 아이템을 '현 시점'에 추천한다.
information retrieval과 ml의 중간 지점. 정보(아이템)을 찾는 과정과 과거 정보를 활용햇 유저의 성향을 배우는 문제.
cf) 예를 들어, 유저A가 높은 평점을 주거나 큰 관심을 가지는 아이템 x와 유사한 아이템 y를 추천한다.


02. 어떤 컨텐츠를 추천할까?
우선, 컨텐츠와 유저 프로필을 파악한다. 파악하는 방법으 수치화 등 정량화하여 비교할 수 있도록 만든다.

Item profiling / User Profiling

- Item Representation : Feature extraction -> Vector Representation
Item profile은 여러 특성들을 나타내며, 이를 정량화해서 사용한다.
구성된 목록 등을 바탕으로 가중치를 부여한다.
ex) attributes, features를 가진다.
item 설명은 text features이므로, text 전처리를 통해 특징을 얻어낸다.

features extraction : 유저의 관심사를 나타내는 많은 정보를 얻고, 만족스러운 추천 결과로 이어진다.
ex) 영화, 음악, 뉴스 등 다양한 feature들을 활용하여 특징을 추출하고 vector화 한다.

item profiling = vector representation ex) onehot vector, dataframe 등으로함.

- User Profile : 평소 사용자의 행동이나 관심을 통해 취향을 얻어낸다. like) 분류 문제와 유사하다
like/dislike, ratings, 댓글, 관심 등을 통해 취향 등으 뽑아낸다.



03.  비슷한 컨텐츠를 찾는 방법

유클리드 거리 : 두 점 사이의 거리를 계산할 때, 사용하는 evaluation metric
여러 차원을 갖는 점과 점 사이의 거리를 계산할 수 있다. (거리 기반의 유사도 측정 방법)

코사인 유사도 : 벡터를 비교할 때, 주로 사용.
예를 들어, vector A와 vector B 사이의 각도로 계산하며, -1 ~ 1 범위의 값을 가지고 -1은 완전한 반대, 0은 서로 독립, 1은 완전히 같은 경우다.

주로, 단순하게 점과 점 사이의 거리만 측정하는 경우 유클리드를 쓰고, 벡터 사이의 각도만을 측정하 때는 코사인 유사도를 쓴다.


04. 근접이웃추천 알고리즘(K-Nearest Neighbor, KNN)

데이터(아이템 또는 유저)로부터 거리가 가까운 k개의 다른 데이터를 선택하는 알고리즘. 주로 ‘유클리드’ 등 거리 계산을 활용함.

회귀(Regression)의 knn 알고리즘
객체의 특성 값이며, 이 값은 k개의 nearest neighbor값들의 평균
ex) f(n)과 f(n+1)의 평균을 구해 줄을 이어가며 예측선을 구한다.

분류(classification)의 knn 알고리즘 = supervised learning
소속된 항목(label)을 출력하며, k개의 nearest neighbor 중 과반수로 분류
ex) 평면 상, 새로운 데이터가 들어온다. 새로 들어온 데이터는 기존 평면 상의 데이터와의 유클리드 거리 계산을 통해 거리를 측정한다. 그 이후, 새로운 점에서 k개를 선정하여 neighbor를 만들고, 다수결의 레이블로 선정한다.


특징 (k값 - hyperparameter)
k는 과반수를 선택해야하기 때문에, 주로 홀수로 하는 것이 일반적.

model-based가 아닌 lazy model. 따로, 학습이 없이 단순 계산을 하는 알고리즘.
k값에 따라, overfit or underfit 이 발생하며, 과도한 정규화로 데이터 label 특성 반영이 어려움.

장점 : 훈련이 없어 빨리 학습할 수 있고, 수치 기반 데이터에서 우수하다. 너무 단순하고 효율적이다.

단점 : 명목(nominal), 더미(dummy) 데이터 처리가 매우 어렵다. 변수가 많아질 수록 처리 속도가 느리고 정확도가 떨어진다.
또한, 적절한 k값 선택이 어렵다. (overfit or underfit 가능성 발생)
무엇보다도, 스케일을 고려하지 않기 때문에 단순 비교가 어렵다.


knn 구현에 앞서 고려해야할 부분
첫 번째로, 변수 간의 분포가 다를 수 있으므로 데이터 정규화가 필요하다.
ex) Z-Score, Min-Max Normalization 등

두 번째로, 데이터 간의 거리 측정 효율화
ex) Locality Sensitive Hashing, Network based Indexer 등

세 번째로, 공간 상의 데이터의 representation 상태를 확인한다. (첫 번째와 유사)
다른 모델에 비해 outlier의 특징이 가능함. 전체적으로 데이터가 주어진 공간 속에서 데이터 특성에 따라 위치와 거리가 적절하게 분포되어있어야만한다.


05. 나이브베이즈 추천 알고리즘
확률(probability)
ex) 사건 A가 발생할 가능성, P(A) = 사건 A의 경우의 수 / 전체 경우의 수


조건부확률(Conditional Probability)
ex) P(A|B) = P(A B) / P(B), P(B)>0
ex) P(A B) = P(A|B)P(B) = P(B|A)P(A)
      = P(A)P(B)


베이즈정리(Bayes’ Theorem)
ex) P(A|B) = P(B|A)P(A) / P(B)
P(A)는 사전확률(prior), P(A|B)는 사후확률(posterior)

이때, 사전확률은 현재 정보를 바탕으로 정한 확률(동전 앞면 나올확률 = 1/2)
사후확률은 사건 B가 사건 A으로부터 발생했다는 가정 하에, 사건 A의 확률 업데이트한다. 베이즈 정리를 통해 사후확률을 계산한다.


나이브 베이즈 분류기 : 베이즈 정리에 의한 통계기반 분류 알고리즘(지도학습)

특징
첫 번째로, 특징(ex feature, attribute 등)끼리 서로 독립이다.
두 번째로, 영화 장르와 영화 감독은 서로 연관이 없다.
세 번째로, 데이터 셋이 커져도 모델 예측에는 관계가 없다.
네 번째로, 연속형 변수보다 이산형 변수에 적합하다.
다섯 번째로, 데이터의 차원이 높아질수록, 모든 class에 대해 확률이 0으로 수렴한다.
ex) laplace smoothing을 활용한다.


나이브베이즈 분류기는 각각의 케이스를 구한 후, 표를 만드는 것이다.


06. TF-IDF란?













